---
title: Discrete Random Variables
date: 2025-09-16/18/23/25
---

## Expectation

For a discrete random variable $X$ with p.m.f. $p_X(t)=\Pr[X=t]$,
$$
\E[X] = \sum_{t p_X(t)\neq 0} t\,p_X(t), 
\qquad 
\E[f(X)] = \sum_{t p_X(t)\neq 0} f(t)\,p_X(t).
$$

**Interpretations**

- Weighted average over values of $X$: $\sum_t t\,\Pr[X=t]$.
- Weighted average over outcomes $\omega$ in the sample space $S$: $\sum_{\omega\in S} X(\omega)\,\Pr(\{\omega\})$.

> Examples  
> - Fair die $X\in\{1,\dots,6\}$: $\E[X]=3.5$.  
> - Heads in three fair flips $Y\in\{0,1,2,3\}$: $\E[Y]=1.5$.  
> - Indicator $\mathbf{1}_A$: $\E[\mathbf{1}_A]=\Pr(A)$.

**Useful summation trick (nonnegative integer-valued $X$)**  
$$
\E[X]=\sum_{k=1}^{\infty}\Pr(X\ge k).
$$

## Linearity of expectation

For random variables $X,Y$ and scalar $a\in\mathbb{R}$,
$$
\E[aX+Y] = a\,\E[X] + \E[Y].
$$
No independence required.

**Counting-by-indicators trick**  
If $X=\sum_{i=1}^n \mathbf{1}_{A_i}$ counts how many events $A_i$ occur, then
$$
\E[X] = \sum_{i=1}^n \Pr(A_i).
$$

> Example: number of fixed points in a random permutation—$\E[X]=1$.

## Variance

Variance measures spread around the mean:
$$
\Var(X)=\E \left[(X-\E X)^2\right]=\E[X^2]-(\E X)^2.
$$

Useful identities:

- For constants $a,b$: $\Var(aX+b)=a^2\,\Var(X)$.
- For an indicator: $\Var(\mathbf{1}_A)=\Pr(A)\big(1-\Pr(A)\big)$.
- If $X=\sum_{i=1}^n \mathbf{1}_{A_i}$ with **independent** indicators, then
$$
\Var(X)=\sum_{i=1}^n \Pr(A_i)\big(1-\Pr(A_i)\big).
$$

## Inclusion–Exclusion via indicators

Using $\mathbf{1}_{A^c}=1-\mathbf{1}_A$ and expanding $\mathbf{1}_{\cup_i E_i}=1-\prod_{i=1}^n(1-\mathbf{1}_{E_i})$, then taking expectations yields
$$
\Pr \Big(\bigcup_{i=1}^n E_i\Big)
=\sum_{k=1}^n (-1)^{k-1} \sum_{1\le i_1<\cdots<i_k\le n} \Pr(E_{i_1}\cap\cdots\cap E_{i_k}).
$$

---

## Bernoulli

If $X\in\{0,1\}$ takes value $1$ with probability $p$, we write $X\sim\mathrm{Ber}(p)$.  
p.m.f.: $p_X(1)=p, p_X(0)=1-p$.  
$\E[X]=p,\quad \Var(X)=p(1-p)$.

---

## Binomial

Let $X$ be the total number of successes in $n$ independent $\mathrm{Ber}(p)$ trials. Then $X\sim \mathrm{Bin}(n,p)$ and $X=X_1+\cdots+X_n$ with $X_i\stackrel{\text{i.i.d.}}{\sim}\mathrm{Ber}(p)$.

p.m.f.:
$$
\Pr(X=k)=\binom{n}{k}p^k(1-p)^{n-k},\qquad k=0,1,\dots,n.
$$

Expectation by linearity:
$$
\E[X]=\sum_{i=1}^n \E[X_i]=np.
$$

Variance—two methods:

1) **Pair expansion**  
$$
\E[X^2]=\E \left(\sum_i X_i^2+2\sum_{i<j}X_iX_j\right)
= np + 2\binom{n}{2}p^2 = np + n(n-1)p^2,
$$
so $\Var(X)=\E[X^2]-(\E X)^2=np(1-p)$.

2) **Factorial moment**  
$\E[X(X-1)]=n(n-1)p^2$, hence $\E[X^2]=n(n-1)p^2+np$ and $\Var(X)=np(1-p)$.

> Examples  
> - Three fair flips: $Y\sim\mathrm{Bin}(3,\tfrac12)$.  
> - Ten dice, number of sixes: $X\sim\mathrm{Bin}(10,\tfrac16)$.  
> - If you win \$5 per six, $Z=5X$ (not binomial). Then
> $$
> \Pr(Z=10)=\Pr(X=2)=\binom{10}{2} \left(\tfrac16\right)^2 \left(\tfrac56\right)^8,\quad
> \E[Z]=5\,\E[X]=\tfrac{25}{3},\quad
> \Var(Z)=25\,\Var(X)=25\cdot 10\cdot\tfrac16\cdot\tfrac56.
> $$

---

## Poisson

A Poisson random variable $X\sim\mathrm{Poi}(\lambda)$ has
$$
\Pr(X=k)=\frac{\lambda^k e^{-\lambda}}{k!},\qquad k=0,1,2,\dots,
$$
which sums to 1 by the series for $e^\lambda$.

Moments via factorial moments:
$$
\E[X]=\lambda,\qquad 
\E\big[X(X-1)\big]=\lambda^2\ \Rightarrow\ \E[X^2]=\lambda+\lambda^2,\quad \Var(X)=\lambda.
$$

### Poisson approximation to Binomial

If $X\sim\mathrm{Bin}(n,p)$ with $n$ large and $p$ small so that $np=\lambda$ (fixed), then for fixed $k$,
$$
\Pr(X=k)=\binom{n}{k}p^k(1-p)^{n-k}\ \approx\ \frac{\lambda^k e^{-\lambda}}{k!}.
$$
Rule-of-thumb: even $n\approx 50$ can already behave “large”.

> Examples  
> - Umbrella with 1000 tiny cells, each hit with prob $5/1000$: $\mathrm{Bin}(1000,0.005)\approx \mathrm{Poi}(5)$.  
> - Gacha with 500 draws, rate $0.6\%$: $\mathrm{Bin}(500,0.006)\approx \mathrm{Poi}(3)$; $\Pr(X=2)\approx 3^2 e^{-3}/2!\approx 11.2\%$.

---

## Geometric

Flip a $p$-biased coin until the **first** head; let $X$ be the total number of flips (including the head). Then $X\sim\mathrm{Geom}(p)$ with support $\{1,2,\dots\}$.

p.m.f.:
$$
\Pr(X=k)=(1-p)^{k-1}p,\qquad k\ge1.
$$

Tail/CDF:
$$
\Pr(X>k)=(1-p)^k,\qquad F_X(k)=1-(1-p)^k.
$$

Expectation (two ways):
$$
\E[X]=\sum_{k\ge1}k\,p(1-p)^{k-1}=\frac1p
\quad\text{and}\quad
\E[X]=\sum_{k\ge1}\Pr(X\ge k)=\sum_{k\ge1}(1-p)^{k-1}=\frac1p.
$$

Second moment and variance:
$$
\E[X^2]=\frac{2}{p^2}-\frac{1}{p},\qquad
\Var(X)=\E[X^2]-(\E X)^2=\frac{1-p}{p^2}.
$$

### Memoryless property

For all $m,n\ge 0$,
$$
\Pr(X>m+n\mid X>m)=\Pr(X>n),
$$
equivalently $\Pr(X>m+n\mid X>m)=\frac{(1-p)^{m+n}}{(1-p)^m}=(1-p)^n$.

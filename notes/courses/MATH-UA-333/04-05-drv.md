---
title: Discrete Random Variables
date: 2025-09-16/18/23/25
---

## Expectation

For a discrete random variable $X$ with p.m.f. $p_X(t)=P[X=t]$,
$$
\E[X] = \sum_{t} t p_X(t), \quad \E[f(X)] = \sum_t f(t) p_X(t),
$$

**Interpretations**

- Weighted average over the values of $X$: $\sum_t t P[X=t]$.
- Weighted average over outcomes $\omega$ in the sample space $S$: $\sum_{\omega\in S} X(\omega) P(\{\omega\})$.

> Examples:
> 
> - Fair die $X\in\{1,\dots,6\}$: $\E[X]=3.5$.
> - Heads in three fair flips $Y\in\{0,1,2,3\}$: $\E[Y]=1.5$.
> - Indicator ${\mathbf{1}}_A$: $\E[{\mathbf{1}}_A]=P(A)$.

## Linearity of expectation

For random variables $X,Y$ and scalar $a\in\mathbb{R}$,
$$
\E[aX+Y] = a \E[X] + \E[Y].
$$
No independence required.

**Counting-by-indicators trick**

If $X=\sum_{i=1}^n {\mathbf{1}}_{A_i}$ counts how many events $A_i$ occur, then

$$
\E[X] = \sum_{i=1}^n P(A_i).
$$

> Example: number of fixed points in a random permutation - $\E[X]=1$.

## Variance

Variance measures spread around the mean:

$$
\operatorname{Var}(X) = \E[(X-\E X)^2] = \E[X^2] - (\E X)^2.
$$

Useful identities:

- For constants $a,b$: $\operatorname{Var}(aX+b)=a^2 \operatorname{Var}(X)$.
- For an indicator: $\operatorname{Var}({\mathbf{1}}_A)=P(A)\big(1-P(A)\big)$.
- If $X=\sum_{i=1}^n {\mathbf{1}}_{A_i}$ with independent indicators, then
  
$$
\operatorname{Var}(X)=\sum_{i=1}^n P(A_i)\big(1-P(A_i)\big).
$$

## Inclusion–Exclusion via indicators

Using ${\mathbf{1}}_{A^c} = 1 - {\mathbf{1}}_A$ and expanding ${\mathbf{1}}_{\cup_i E_i} = 1 - \prod_{i=1}^n (1-{\mathbf{1}}_{E_i})$, then taking expectations yields
  
$$
P\Big( \bigcup_{i=1}^n\ {\E}_i \Big)
= \sum_{k=1}^n (-1)^{k-1}
\sum_{1 \leq i_1 < \cdots < i_k \leq n}
P(\E_{i_1} \cap \cdots \cap \E_{i_k}).
$$

### Bernoulli

If $X\in\{0,1\}$ takes value $1$ with probability $p$, we write $X\sim\operatorname{Ber}(p)$.  Its pmf is $p_X(1)=p$, $p_X(0)=1-p$; the expectation is $p$, and the variance is $p(1-p)$.

### Binomial

Let $X$ be the total number of successes in $n$ independent Bernoulli$(p)$ trials.  Then $X\sim \operatorname{Bin}(n,p)$ and can be written $X=X_1+\cdots+X_n$ with $X_i\sim\operatorname{Ber}(p)$. The pmf is

$$
P(X=k) = {n\choose k} p^k (1-p)^{ n-k}.
$$

Using linearity of expectation, $\E[X]=\sum_{i=1}^n \E[X_i]=np$, and the variance is $\operatorname{Var}(X)=np(1-p)$.
> Examples: the number of heads in 3 fair flips follows $\operatorname{Bin}(3,\tfrac12)$; when rolling 10 dice, the number of sixes follows $\operatorname{Bin}(10,\tfrac16)$.

### Poisson

A **Poisson** random variable $X\sim \operatorname{Poi}(\lambda)$ has pmf
$$
P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k=0,1,2,\dots.
$$

From the series expansion of $e^\lambda$, these probabilities sum to one. The expectation and variance are both $\lambda$. A Poisson variable arises as an approximation to $\operatorname{Bin}(n,p)$ when $n$ is large and $p$ is small with $np=\lambda$. 

> For example, the number of 5‑star characters obtained in 500 draws with a 0.6% rate is approximately $\operatorname{Poi}(3)$.

### Geometric

If you repeatedly flip a $p$‑biased coin until the first head appears, the total number of flips $X$ (including the successful flip) follows a **geometric** distribution with parameter $p$, denoted $X\sim \operatorname{Geom}(p)$.  The pmf is
$$
P(X=k) = (1-p)^{k-1} p,\quad k=1,2,\dots,
$$

since the first $k-1$ flips must be tails and the $k$‑th flip a head. Summation identities yield
$$
\E[X] = \frac{1}{p},\qquad \operatorname{Var}(X) = \frac{1-p}{p^2}.
$$

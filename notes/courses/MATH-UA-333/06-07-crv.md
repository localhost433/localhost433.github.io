---
title: Continuous Random Variables
date: 2025-09-29|10-07/09/14
---

## Definition

A random variable $X$ is **continuous** if there exists a probability density function (p.d.f.) $f_X(t)$ such that for all $-\infty \le a < b \le \infty$,
$$
P(a \le X \le b) = \int_a^b f_X(t)dt,
$$
and, more generally, for any measurable $A \subset \mathbb{R}$,
$$
P(X \in A) = \int_A f_X(t)dt.
$$

**Basic properties**
- Normalization:
  $$
  \int_{-\infty}^{\infty} f_X(t)dt = 1.
  $$
- Cumulative distribution function (c.d.f.):
  $$
  F_X(x) = P(X \le x) = \int_{-\infty}^{x} f_X(t)dt,
  \qquad
  \text{and } F_X'(t) = f_X(t) \text{ for almost every }t
  $$
- Nonnegativity and point probabilities:
  $$
  f_X(t) \ge 0 \quad \text{for all } t,\quad
  P(X=a)=0 \quad \text{for any fixed } a.
  $$
  A density may exceed $1$ on a short interval (e.g., uniform on $[0,\tfrac12]$ has height $2$), since only the integral must equal $1$.

---

## Expectation and variance

For any (integrable) function $g$,
$$
E[X] = \int_{-\infty}^{\infty} tf_X(t)dt,
\qquad
E[g(X)] = \int_{-\infty}^{\infty} g(t)f_X(t)dt.
$$

Linearity (no independence needed):
$$
E[aX + bY] = aE[X] + bE[Y].
$$

Variance:
$$
\operatorname{Var}(X) = E\big[(X - E[X])^2\big] = E[X^2] - (E[X])^2.
$$

**General workflow for problems**  
1. Find $F_X$ first, then differentiate to get $f_X$, or derive $f_X$ directly.
2. Compute probabilities via integrals of $f_X$.
3. Compute $E[g(X)]$ via $\int gf$; use symmetry, substitution, or parts as needed.

---

## Worked example (polynomial density)

Let
$$
f_X(t) =
\begin{cases}
C(4t - 2t^2), & 0 < t < 2,\\
0, & \text{otherwise.}
\end{cases}
$$
Normalization gives
$$
1 = \int_0^2 C(4t - 2t^2)dt
= C\Big[2t^2 - \tfrac{2}{3}t^3\Big]_0^2
= C\cdot \tfrac{8}{3}
\Rightarrow
C=\tfrac{3}{8}.
$$
Then
$$
P(X \le 1)
= \int_0^1 \tfrac{3}{8}(4t - 2t^2)dt
= \tfrac{3}{8}\Big[2t^2 - \tfrac{2}{3}t^3\Big]_0^1
= \tfrac{1}{2}.
$$

---

## Transformations

If $Y = g(X)$ with $g$ strictly monotone and continuously differentiable, then
$$
f_Y(y) = f_X\big(g^{-1}(y)\big)\frac{d}{dy}g^{-1}(y).
$$
If $g$ is decreasing, the same formula holds with the absolute value of the derivative.

**Non-monotone example $g(t)=t^2$**  
- Support: $Y \ge 0$.
- For $y>0$, the preimages are $\sqrt{y}$ and $-\sqrt{y}$:
  $$
  f_Y(y) = \frac{1}{2\sqrt{y}}\Big(f_X(\sqrt{y}) + f_X(-\sqrt{y})\Big), \quad y>0,
  \qquad f_Y(y)=0 \text{ for } y\le 0.
  $$

**Example: $X\sim \text{Unif}(0,1)$, $Y=X^2$**  
$$
E[Y]=E[X^2]=\int_0^1 t^2dt=\tfrac{1}{3},
\qquad
f_Y(y)=\frac{1}{2\sqrt{y}},0<y<1,\text{else }0.
$$

---

## Uniform distribution

If $X\sim \text{Unif}([a,b])$ with $a<b$,
$$
f_X(t)=\frac{1}{b-a}\text{ for } a\le t\le b,\quad 0 \text{ otherwise,}
\qquad
F_X(t)=
\begin{cases}
0,& t\le a,\\
\dfrac{t-a}{b-a},& a<t<b,\\
1,& t\ge b,
\end{cases}
$$
$$
E[X]=\frac{a+b}{2},
\qquad
\operatorname{Var}(X)=\frac{(b-a)^2}{12}.
$$
Affine invariance: if $Y=(b-a)Z+a$ and $Z\sim \text{Unif}([0,1])$, then $Y\sim \text{Unif}([a,b])$.

**Arrival example (uniform in phase)**  
If trains arrive every $6$ minutes and arrival time modulo $6$ is uniform on $[0,6]$, then the waiting time $Y\sim \text{Unif}([0,6])$ so $E[Y]=3$ minutes.

---

## Normal (Gaussian) distribution

$X\sim N(\mu,\sigma^2)$ has
$$
f_X(t)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{(t-\mu)^2}{2\sigma^2}\Big),
$$
which integrates to $1$ (via the Gaussian integral and change of variables).

**Linear transformations and standardization**  
If $Y=aX+b$, then $Y\sim N(a\mu+b,a^2\sigma^2)$.
Define the standard normal $\xi=\dfrac{X-\mu}{\sigma}\sim N(0,1)$, so $X=\sigma\xi+\mu$.

**Mean and variance**  
Using symmetry and integration by parts for $\xi\sim N(0,1)$,
$$
E[\xi]=0,\qquad E[\xi^2]=1
\Rightarrow
E[X]=\mu,\quad \operatorname{Var}(X)=\sigma^2.
$$

**C.D.F. and symmetry**  
Denote $\Phi(x)=P(\xi\le x)=\int_{-\infty}^x \tfrac{1}{\sqrt{2\pi}}e^{-s^2/2}ds$.
Then
$$
F_X(t)=P(X\le t)=\Phi\Big(\frac{t-\mu}{\sigma}\Big),\qquad
\Phi(-x)=1-\Phi(x).
$$

**Example**  
If $X\sim N(3,9)$, then $P(X>0)=\Phi(1)\approx 0.8413$ and
$$
P(|X-3|>6)=P(|\xi|>2)=2\big(1-\Phi(2)\big)\approx 0.0456.
$$

---

## Exponential distribution

$X\sim \operatorname{Exp}(\lambda)$ with $\lambda>0$ has
$$
f_X(t)=\begin{cases}
\lambda e^{-\lambda t},& t>0,\\
0,& t\le 0,
\end{cases}
\qquad
F_X(t)=\begin{cases}
0,& t\le 0,\\
1-e^{-\lambda t},& t>0,
\end{cases}
$$
$$
P(X\ge t)=e^{-\lambda t},\qquad
E[X]=\frac{1}{\lambda},\qquad
\operatorname{Var}(X)=\frac{1}{\lambda^2}.
$$

**Memoryless property**  
For $s,t>0$,
$$
P(X\ge s+t\mid X\ge s)=\frac{P(X\ge s+t)}{P(X\ge s)}=e^{-\lambda t}=P(X\ge t).
$$

**Example**  
If the mean gap is $6$ minutes, then $\lambda=1/6$ and
$$
P(X\ge 12)=e^{-2}.
$$

---

## Links to discrete models

- **Geometric $\to$ Exponential**: with time grid of size $1/n$ and success probability $\lambda/n$ per tick, the first success time $X_n$ satisfies
  $$
  P(X_n>t) = \big(1-\tfrac{\lambda}{n}\big)^{\lfloor nt\rfloor} \to e^{-\lambda t},
  $$
  so $X_n \Rightarrow \operatorname{Exp}(\lambda)$.
- **Binomial $\to$ Poisson counts**: the number of successes in $[0,t]$ with per-tick success probability $\lambda/n$ is $\operatorname{Bin}(\lfloor nt\rfloor,\lambda/n)\Rightarrow \operatorname{Poi}(\lambda t)$.


---
title: Loss-free Balancing in MoEs (2)
date: 2025-07-06
tags: [cs, finance, ai, notes]
author: R
location: Above around Ust-Ilimsk, Russia while on a plane from New York to Hong Kong
---

(On second thought) I should've just annotated the paper instead of writing this. Reading something this bright on an airplane with the lights off felt like some type of torture. I couldn't fall asleep in my seat right away, and when I woke up the screen was still on; that made me feel that I had to finish it before doing anything else.

## Problem: Imbalanced routing (Continued)
The problem was illustrated in [the last post](https://robinc.vercel.app/post.html?id=011-MoE1), Italics are directly quoted from the paper.

### Solution: EC (Expert Choice)
The authors of the [2025 ICLR paper](https://arxiv.org/pdf/2408.15664) indicated that this approach *breaks the causal constraint* (causing a leakage of future information which *destroys the generalization of a model and prevents reliable evaluation*). They even proved their hypothesis (*that the loss drop originates from the model’s accessing and exploiting future token information*) through experiments.

### Solution: Loss-Free Balancing
We adjust each expert’s bias $b_i$ online to enforce perfectly balanced routing. Algorithm 1 (Wang et al., 2025) shows how:

Adjusting the per-expert bias $b_i$ during training
Input: MoE model $\theta$, batch iterator $\mathcal{B}$, bias-update rate $\mathbf{u}$

1. Initialize $b_i \leftarrow 0, \forall i \in \{1 \dots N\}$.
2. For each batch ${(x_k,y_k)}_k \in \mathcal{B}$:
> - Compute raw gating scores
> $$
> s_{i,t} = G(\textbf{u}_t^\top \textbf{e}_i)
> $$
> - Prune with bias

$$
\begin{align*}
    g_{i,t} &= 
        \begin{cases}
            s_{i,t}, & s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \leq j \leq N\} , K) \\\\
            0, & \text{otherwise}
        \end{cases}
\end{align*}
$$

> > Note that $b_i$ only take part in Top-k but doesn't contribute to the gating score.
> - Train $\theta$ on this batch using weights $g_{i,t}$.
> - Count assignments:
> $$
> \begin{align*}
>     c_i = \sum_{t=1}^T \mathbb{1} &, \quad \bar{c} = \frac{KT}{N}\\
>     e_i &= \bar{c} - c_i
> \end{align*}
> $$
> - Update bias:
> $$
> b_i \leftarrow b_i + u \cdot \sgn(\mathbf{e}_i)
> $$
3. Return $\theta$ and $b_i$.

No extra loss term is needed—by pushing $b_i$ up or down by the sign of the load violation, the expected fraction of tokens per expert is driven to $1/N$.

## MoE in Application
And there's [the 2023 paper](https://personal.ntu.edu.sg/boan/papers/KDD23_Stock.pdf) (Sun et al., 2023) which builds a MoE in order to account for multiple metrics, which differs from mainstream deep learning models being used in the industry.

## For me
I guess my main task is to apply Loss-Free Balancing to an MoE model. Because just looking at the time of the publication and practices around, I'd say the majority of the industry is still using deep learning models that are dependent on other architectures. While attempts could be made (and I'm sure there have been such attempts all around) to take in more indicators, it seems that building a MoE model which overcomes drawbacks in auxiliary-loss implementations could be advantageous.
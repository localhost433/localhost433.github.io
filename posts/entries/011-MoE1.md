---
title: Auxiliary-loss Load Balancing in MoEs (1)
date: 2025-07-06
tags: [cs, ai, notes]
author: R
location: Above Illulissat, Greenland while on a plane from New York to Hong Kong
---

I'm currently reading [this conference paper for ICLR 2025](https://arxiv.org/pdf/2408.15664) (Wang et al., 2025) as I'm preparing for my internship, but after going through the intro I'd like to take down some notes, as there are a lot of ideas and lessons that I've learned while reading it. Italics in this note are directly quoted from the paper.

## MoEs
After opening the paper I encountered the concept of MoEs. To get myself more familiar, I read [this blog on Hugging Face](https://huggingface.co/blog/moe) (Sanseviero et al., 2023), which was really helpful—highly recommended. MoE stands for **M**ixture **o**f **E**xperts; a famous example of its type is DeepSeek. It has many advantages, as the authors wrote: easy to scale to a large number of parameters, manageable costs, etc.

*Let $u_t$ denote the input of the $t$-th token to an $N$-expert MoE layer, the output $h_t$ is computed as follows:*
Let  
- $N$ be the number of experts,  
- $K$ the number of experts selected per token,  
- $T$ the total number of tokens in the batch,  
- $\mathbf{u}_t\in\R^d$ the input for token $t$,  
- $\text{FFN}_i: \R^d\to\R^d$ the $i$-th expert network,  
- $e_i\in\R^d$ the centroid (parameter) of expert $i$, and  
- $G\colon\R\to\R_{>0}$ a positive gating function (e.g. $\exp$, $\text{sigmoid}$, or $\text{softmax}$).
- $s_{i,t}$ is the *raw gating score* for expert $i$ on token $t$, obtained by applying $G$ to the dot‐product of input $\mathbf{u}_t$ and expert centroid $e_i$.  
- $g_{i,t}$ is the *pruned gating weight*: it equals $s_{i,t}$ if $s_{i,t}$ ranks among the top-$K$ scores for token $t$, and zero otherwise.

Compute for each token $t$ and expert $i$:
$$
\begin{align*}
    g_{i,t} &= 
        \begin{cases}
        s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N\} , K)\\\\
                 &          (\text{if $s_{i,t}$ is among the top-$K$ scores})\\\\
        0, & \text{otherwise}
        \end{cases} \\\\
    s_{i,t} &= G(\textbf{u}_t^\top e_i)
\end{align*}
$$

and form the layer output

$$
\textbf{h}_t = \textbf{u}_t + \sum^N_{i=1} g_{i,t} \text{FFN}_i (\textbf{u}_t)
$$

So here $G$ could be any function $\R \to \R_{>0}$. Some conventional ones could be $\exp$, softmax, or sigmoid (to be honest I had to look these two up to see what they are exactly). In this paper they use the latter two.

And there is the expert consulted following the gating function.

## Problem: Imbalanced routing
But one problem MoEs often experience is imbalanced routing (a small number of experts receive most tokens), thus creating *a risk of routing collapse (Shazeer et al., 2017), where the model consistently selects only a few experts, hindering sufficient training of the other experts*, or a *computational bottleneck due to load imbalance*.

I was wondering how it could cause a computational bottleneck, but then I realized the way I thought about it—that it could easily scale through parallelism or other ways—is not easily achievable. Since there are different machines hosting each expert, it depends more on the load given to a certain expert.

Plus, the training loop would need a substantial redesign to use the idle computational power to catch up. Even if I create replicas for the "hot" experts on more hosts, they need to be in sync, which creates a lot of cost by itself. Merging gradients across replicas requires collective operations every step; at that point it will just recreate the original problem we’re trying to overcome if one of these slows down...

### Solution: Auxiliary-loss
To address this issue, there is an auxiliary loss that encourages balanced load and thus avoids imbalanced routing in training MoEs. To do this, it penalizes the use of only a few experts. It’s mostly within the process of the gating function. 

**Key variables:**
- $N$: number of experts in the MoE layer
- $K$: number of experts selected per token (top-K)  
- $T$: total number of tokens in the batch
- $\mathbb{1}$: indicator function (equals 1 if condition is true, 0 otherwise)
- $\alpha$: balancing‐loss weight (manually set hyperparameter)

Defined as such:

- **Normalized load**   
  $f_i$:= the fraction of tokens routed to expert $i$:
  $$
    f_i = \frac{N}{KT} \sum_{t=1}^T \mathbb{1} (i \in \text{Topk} \mid \mathbf{u}_t )
  $$

- **Average gating weight**  
  $P_i$:= the mean score assigned by the gate to expert $i$:
  $$
    P_i = \frac{1}{T} \sum_{t=1}^T s_{i,t}
  $$

Combine these into a single penalty term:

$$\mathcal{L}_{\mathrm{balance}} = \alpha \sum_{i=1}^N f_i P_i$$


**Regularization terms:**  
Introduce two small-weight penalties on the imbalance of $\{P_i\}$ and $\{f_i\}$:

\begin{align*}
  \mathcal{L}_P &= \lambda_P \operatorname{CV}^2({P_i}) \\\\
  \mathcal{L}_f &= \lambda_f \operatorname{CV}^2({f_i})
\end{align*}

where typically $\lambda_{P} \approx \lambda_{f} \sim 10^{-2}$.

> This is actually optional, for simpler just use $\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \mathcal{L}_{\text{balance}}$.  
> I write it this way just to follow the [original MoE auxiliary-loss formulation paper (Shazeer et al. (2017))](https://arxiv.org/pdf/1701.06538).


**Imbalance metric: coefficient of variation squared**  
For any set of scalars $\{z_i\}_{i=1}^N$, define

$$
  \text{CV}^2(\{z_i\}) =
  \frac{\frac{1}{N} \sum_{i=1}^N z_i^2 - (\frac{1}{N} \sum_{i=1}^N z_i )^2}
       {(\frac{1}{N} \sum_{i=1}^N z_i )^2},
$$

which satisfies $\text{CV}^2=0$ exactly when all $z_i$ are equal.

> By the way, this looks very much like the variance.  
> Write $\mu = \tfrac1N \sum_i z_i$ and $\nu = \tfrac1N \sum_i z_i^2$. Then
> $$
> \text{CV}^2 = \frac{\nu - \mu^2}{\mu^2} = \frac{\text{Var}}{(\text{Mean})^2}
> $$
> Its partial derivative w. one coordinate $z_k$ is
> $$
> \frac{\partial \text{CV}^2}{\partial z_k}
> = \frac{2}{N}\Bigl(\frac{z_k}{\mu^2} - \frac{\nu}{\mu^3}\Bigr).
> $$
> > Details:
\begin{align*}
  \frac{\partial}{\partial z_k} (\tfrac{\nu - \mu^2}{\mu^2})
  &= \frac{1}{\mu^2} \frac{\partial\nu}{\partial z_k} - \frac{\nu - \mu^2}{\mu^4} 2\mu \frac{\partial\mu}{\partial z_k}\\\\
  &= \frac{1}{\mu^2} \frac{2z_k}{N} - \frac{\nu - \mu^2}{\mu^4} \frac{2\mu}{N}\\\\
  &= \frac{2}{N}\Bigl(\frac{z_k}{\mu^2} - \frac{\nu}{\mu^3}\Bigr).
\end{align*}
> 
> Because $\nu/\mu^3$ is the same constant for all $k$, this gradient pushes down any $z_k > \mu$ (overloaded expert) and pushes up any $z_k < \mu$ (underloaded expert). In other words, the derivative of a variance term normalized by $\mu^2$.


**Total training objective**  
Combine with the primary task loss $L_{\text{task}}$:
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \mathcal{L}_{P} + \mathcal{L}_{f}.$$ 

#### Intuition
- The penalty grows as either $f_i$ or $P_i$ grows (since it's a product). Then the routing distribution is driven toward uniformity by the penalties. Backpropagation through the parameters plays a role in this process.
- Minimizing $\text{CV}^2$ drives the variance of $\{\text{Imp}_i\}$ or $\{\text{Load}_i\}$ toward zero relative to their mean (see derivation of $\partial \text{CV}^2/\partial z_k$ above).
- Any expert $i$ with above-average usage raises its own $\text{Imp}_i$ or $\text{Load}_i$, increasing the penalty.


#### Drawbacks
The ICLR 2025 paper mentioned that auxiliary loss might introduce unwanted gradients, as the MoE models perform worse on some metrics.

However, I wasn't really convinced by this reasoning. The performance was not improved that significantly (I was expecting a larger gap) for the validation perplexity. There's a bunch of other models they could choose from, but instead they picked this small one. The load balance one sounds okay, and that's the main point of the paper, so it's good.

The true drawback, in my opinion, comes with the act of rebalancing through auxiliary loss itself.
- The idea of MoE is having many highly specialized experts; auxiliary loss fights any concentration of weight, even if that concentration was beneficial for modeling those tokens.
- The balancing gradient for an expert involves all experts' totals. So updating the logit for one expert now depends on every other expert’s load. It's obvious it can drown out more specialized signals.
- Naturally, experts that are good at certain tokens are expected to get those; trying to make the router equalize loads regardless of quality can route a token to a weaker expert, simply because the "best" expert is already slightly busier.

(TBC)

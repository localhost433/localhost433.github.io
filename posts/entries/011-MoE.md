---
title: Loss-free Balancing in MoEs
date: 2025-07-06
tags: [cs, finance, ai]
author: R
location: Above Illulissat, Greenland while on a plane from New York to Hong Kong
---

I'm currently reading [this conference paper for ICLR 2025](https://arxiv.org/pdf/2408.15664) (Wang et al., 2025) as I'm preparing for my internship, but after gone through the intro I'd like to take down some notes, as there are a lot of ideas, lessons that I've learned while reading it. Italics in this note is directly quoted from the paper.

## MoEs
After opening the paper I encountered the concept of MoEs, to get myself more familiar, I have read [this blog on HuggingFace](https://huggingface.co/blog/moe) (Sanseviero et al., 2023) which was really helpful, highly recommended. MoE accounts for **M**ixture **o**f **E**xperts, a famous example of it's type is Deepseek. It have many advantages, as the authors have wrote, easy to scale to a large amount of parameters, managable costs, etc.

*Let $u_t$ denote the input of the $t$-th token to an $N$-expert MoE layer, the output $h_t$ is computed as follows:*
$$
\begin{align*}
        \textbf{h}_t &= \textbf{u}_t + \sum^N_{i=1} g_{i,t} \text{FFN}_i (\textbf{u}_t) \\
    g_{i,t} &= 
        \begin{cases}
        s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N\} , K) \\
        0, & \text{otherwise}
        \end{cases}\\
    s_{i,t} &= G(\textbf{u}_t^\top e_i)
\end{align*}
$$
*where $G$ is a nonlinear gating function and $e_i$ is the centroid of the $i$-th expert.*

So, here $G$ could be something that is $\R \to \R_{>0}$, some conventional ones could be a $\exp$, softmax or sigmoid (TBH I have to search these up to see what they actually are). In this paper they have use the latter 2.

And there is the Expert consulted following the gating function.

### Problem: Inbalanced routing
But one problem MoEs often experience is inbalanced routing (a few number of experts recieve the most token), thus *a risk of routing collapse (Shazeer et al., 2017), where the model consistently selects only a few experts, hindering sufficient training of the other experts*; or, a *computational bottleneck by load inbalance*.

I was wondering how it could cause a computational bottleneck, but then I realized the way I thought about it that it could easily scale through parallelism or some other ways is not easily achievable. Since there are different machines hosting each model, it depends more on the load given to a certain expert.

Plus, the training loop should undergo a substantial redesign for it to use the idle computational power to catch up. Even if I create replicas for the "hot" experts on more hosting devices, they need to be in sync, therefore creating a lot of cost by itself. Merging gradients across replicas requires collective operations every step, at that point it will just recreate the original problem trying to overcome if 1 of these slowed down...

#### Solution: Auxiliary-loss
To address this issue, there is auxiliary-loss encourage balanced load thus avoids inbalanced routing in training MoEs. To do this, it penalized the use of only a few number of agents. Defined as such:

$$
\begin{align*}
    \mathcal{L}_\text{Balance} &= \alpha \sum^N_{i=1}f_i P_i, \\
    f_i &= \frac{N}{KT} \sum^T_{t=1} \mathbb{1} \text{  (Token t selects Expert i)}, \\
    P_i &= \frac{1}{T} \sum^T_{t=1} s_{i,t}
\end{align*}
$$

Basically, divide by the variance. 

#### Solution: EC (Expert Choice)
The authors of the paper have indicated that this approach *break the causal constraint* (causing a leakage in future information which *destroys the generalization of a model and prevents reliable evaluation*). They have even proved their hypothesis (*that the loss drop originates from the modelâ€™s accessing and exploiting future token information*) through experiment:
Theoretically

#### Solution: Loss-Free Balancing
This approach is what the authors of this paper have come up with. Essentially introducing a bias factor without adding some noisy gradients like the Auxiliary-loss one does.

## MoE in Trading
And there's [the 2023 paper](https://personal.ntu.edu.sg/boan/papers/KDD23_Stock.pdf) (Sun et al., 2023) which builds a MoE in order to account for multiple metrics, which differs from mainstream deep learning models being used in the industry.

## For me
I guess my main task is that to apply Loss-Free Balancing for an MoE model.

Because just looking at the time of the publication and practices around I'd say the majority of the industry is still using deep learning models that are dependent on a single indicator.

While attempts could be made (and I'm sure there have been such attempts all around) to take in more indicators, it seems like that building a MoE model which overcomes drawbacks in auxiliary-loss implementations could be advantageous.

On second thought, I should've just annotated the paper instead of writing this. Reading sth. as this bright on an airplane with the lights off felt like some type of torture. I couldn't fall asleep in the tiny space, and when I woke up the screen is still on, that made me felt that I have to finish it before doing anything else.
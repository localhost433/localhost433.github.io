---
title: Auxiliary-loss Load Balancing in MoEs (1)
date: 2025-07-06
tags: [cs, finance, ai]
author: R
location: Above Illulissat, Greenland while on a plane from New York to Hong Kong
---

I'm currently reading [this conference paper for ICLR 2025](https://arxiv.org/pdf/2408.15664) (Wang et al., 2025) as I'm preparing for my internship, but after gone through the intro I'd like to take down some notes, as there are a lot of ideas, lessons that I've learned while reading it. Italics in this note is directly quoted from the paper.

## MoEs
After opening the paper I encountered the concept of MoEs, to get myself more familiar, I have read [this blog on HuggingFace](https://huggingface.co/blog/moe) (Sanseviero et al., 2023) which was really helpful, highly recommended. MoE accounts for **M**ixture **o**f **E**xperts, a famous example of it's type is Deepseek. It have many advantages, as the authors have wrote, easy to scale to a large amount of parameters, managable costs, etc.

*Let $u_t$ denote the input of the $t$-th token to an $N$-expert MoE layer, the output $h_t$ is computed as follows:*
Let  
- $N$ be the number of experts,  
- $K$ the number of experts selected per token,  
- $T$ the total number of tokens in the batch,  
- $\mathbf{u}_t\in\R^d$ the input for token $t$,  
- $\mathrm{FFN}_i: \R^d\to\R^d$ the $i$-th expert network,  
- $e_i\in\R^d$ the centroid (parameter) of expert $i$, and  
- $G\colon\R\to\R_{>0}$ a positive gating function (e.g. $\exp$, $\mathrm{sigmoid}$, or $\mathrm{softmax}$).

Compute for each token $t$ and expert $i$:
$$
\begin{align*}
    g_{i,t} &= 
        \begin{cases}
        s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N\} , K)\\
                 &          (\text{if $s_{i,t}$ is among the top-$K$ scores})
        \\
        0, & \text{otherwise}
        \end{cases}\\
    s_{i,t} &= G(\textbf{u}_t^\top e_i)
\end{align*}
$$

and form the layer output
$$
\textbf{h}_t = \textbf{u}_t + \sum^N_{i=1} g_{i,t} \text{FFN}_i (\textbf{u}_t) \\
$$

So, here $G$ could be something that is $\R \to \R_{>0}$, some conventional ones could be a $\exp$, softmax or sigmoid (TBH I have to search these up to see what they actually are). In this paper they have use the latter 2.

And there is the Expert consulted following the gating function.

### Problem: Inbalanced routing
But one problem MoEs often experience is inbalanced routing (a few number of experts recieve the most token), thus *a risk of routing collapse (Shazeer et al., 2017), where the model consistently selects only a few experts, hindering sufficient training of the other experts*; or, a *computational bottleneck by load inbalance*.

I was wondering how it could cause a computational bottleneck, but then I realized the way I thought about it that it could easily scale through parallelism or some other ways is not easily achievable. Since there are different machines hosting each model, it depends more on the load given to a certain expert.

Plus, the training loop should undergo a substantial redesign for it to use the idle computational power to catch up. Even if I create replicas for the "hot" experts on more hosting devices, they need to be in sync, therefore creating a lot of cost by itself. Merging gradients across replicas requires collective operations every step, at that point it will just recreate the original problem trying to overcome if 1 of these slowed down...

#### Solution: Auxiliary-loss
To address this issue, there is auxiliary-loss encourage balanced load thus avoids inbalanced routing in training MoEs. To do this, it penalized the use of only a few number of agents. Its mostly within the process of the gating function. Defined as such:



- **Normalized load**  
  $$
    f_i = \frac{N}{KT} \sum_{t=1}^T \mathbb{1} (i \in \mathrm{Topk} \mid \mathbf{u}_t )
    \quad (\text{fraction of tokens routed to expert }i ).
  $$
- **Average gating weight**
  $$
    P_i = \frac{1}{T}\sum_{t=1}^T s_{i,t}
    \quad (\text{mean score assigned by the gate to expert }i ).
  $$
`
##### Balance loss

Combine these into a single penalty term:
$$
  \mathcal{L}_{\mathrm{balance}}
   = 
  \alpha \sum_{i=1}^N f_i P_i.
$$

##### Why this encourages balanced routing

- $f_i$ captures how heavily expert $i$ is used, while $P_i$ captures its average gate score.  
- If an expert is over-selected ($f_i$ large), the product $f_iP_i$ grows, increasing the penalty.  
- Gradients then adjust the gating parameters to **decrease** routing to over-used experts and **increase** routing to under-used ones, driving the distribution toward uniformity.

##### Why this balances load

- Minimizing $\mathrm{CV}^2$ drives the variance of $\{\mathrm{Imp}_i\}$ or $\{\mathrm{Load}_i\}$ toward zero *relative* to their mean.  
- Any expert $i$ with above-average usage raises its own $\mathrm{Imp}_i$ or $\mathrm{Load}_i$, increasing the penalty.  
- Backpropagation through the gating parameters encourages **reduced** routing to overloaded experts and **increased** routing to underutilized ones, leading to a more uniform expert selection distribution.  

Basically, divide by the variance.

(TBC)